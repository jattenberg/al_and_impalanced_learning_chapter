\section{Active Learning for Imbalanced Problems}
\label{sec:related}
The intent of this section is to provide the reader with some background on the active learning problem in the context of building cost-effective classification models. We then discuss challenges encountered by active learning heuristics in settings with significant class imbalance. We then discuss strategies specialized in overcoming the difficulties imposed by this setting.


\subsection{Background on Active Learning}
\label{sec:background}
Active Learning is specialized set of machine learning techniques developed for reducing the annotation costs associated with gathering the training data required for building predictive statistical models. In many applications, {\em unlabeled} data comes relatively cheaply when compared to the costs associated with the  acquisition of a ground-truth value of the target variable of that data. For instance, the textual content of a particular web page may be crawled readily, or the actions of a user in a social network may be collected trivially by mining the web logs in that network. However, knowing with some degree of certainty the topical categorization of a particular web page, or identifying any malicious activity of a user in a social network is likely to require costly editorial review. These costs restrict the number of examples that may be labeled, typically to a small fraction of the overall population. Because of these practical constraints typically placed on the overall number of ground truth labels available, and the tight dependence of the performance of a predictive model on the examples in its training set, the benefits of careful selection of the examples are apparent. This importance is further evidenced by the vast research literature on the topic.

While an in-depth literature review is beyond the scope of this chapter, for context we provide a brief overview of some of the more broadly cited approaches in active learning. For a more thorough treatment on the the history and details of active learning, we direct the reader to the excellent survey by Settles~\cite{settles.tr09}. Active learning tends to focus on two scenarios--- (i) stream-based selection, where unlabeled examples are presented one-at-a-time to a predictive model, which feeds predicted target values to a consuming process and subsequently applies an active learning heuristic to decide if some budget should be expended gathering this example's class label for subsequent re-training. (ii) pool-based active learning, on the other hand, is typically an offline, iterative process. Here, a large set of unlabeled examples are presented to an active learning system. During each epoch of this process, the active learning system chooses one or more unlabeled examples for labeling and subsequent model  training. This proceeds until the budget is exhausted or some stopping criterion is met. At this time, if the predictive performance is sufficient, the model may be incorporated into an end system that feeds the model with unlabeled examples and consumes the predicted results. A diagram illustrating these two types of active learning scenarios is presented in Figures~~\ref{fig:streambased} and \ref{fig:poolbased} respectively. Due to the greater attention given to pool-based active learning in recent scientific literature and the additional power available to an active learning system capable of processing a large representation of the problem space at once, the remainder of this chapter focuses on the latter of these two scenarios, the pool-based setting.

\begin{figure}[hbt]
\begin{center}
\epsfig{figure=plots/poolBasedActiveLearning.eps,width=.75 \columnwidth}
\end{center}
\caption{Pool-Based Active Learning}
\label{fig:poolbased}
\end{figure}

\begin{figure}[hbt]
\begin{center}
\epsfig{figure=plots/onlineActiveLearning.eps,width=.75 \columnwidth}
\end{center}
\caption{Stream-Based Active Learning}
\label{fig:streambased}
\end{figure}


The most common techniques in active learning have focused on selecting examples from a so-called ``region of uncertainty'', the area nearest to the current model's predictive decision boundary.\footnote{For instance, the simplest case when performing binary classification would involve choosing $x' = \arg\max_x \min_y P(y | x), y \in {0,1}$. } Incorporating uncertainty into active data acquisition dates back to research focused on optimal experimental design~\cite{federovOptimal1972}, and has been amongst the earliest successful examples of active machine learning techniques~\cite{cohn1992:activeLearning, lewis94sequential}. The intuition behind uncertainty-based selection is that this region surrounding a model's decision boundary is where that model is most likely to make mistakes. Incorporating labeled examples from this region may improve the model's performance along this boundary, leading to gains in overall accuracy.

Many popular subsequent techniques are specializations of uncertainty selection, including query-by-committee-based approaches~\cite{Freund94siftinginformative,Dagan95committeebasedsampling,Freund:1993:qbc}, where, given an ensemble of (valid) predictive models, examples are selected based on the level of disagreement elicited amongst the ensemble, and the popular ``simple margin'' technique proposed by Tong and Koller~\cite{tong02svm}, where, given a current parameterization of a support vector machine, $w_j$, the example $x_i$  is chosen that comes closest to the decision boundary, $x_i = \arg \min_{x'} |w_j \Phi(x)|$, where $\Phi( \cdot )$ is a function mapping an example to an alternate space utilized by the kernel function in the support vector machine: $k(u,v) = \Phi(u)\Phi(v)$.

Expected-utility-based approaches, where examples are chosen based on the estimated expected improvement in a certain objective achieved by incorporating a given example into the training set.\footnote{Note that this selection objective may not necessarily be the same objective used during the base model's use time. For instance, examples may be selected according to their contribution to the reduction in problem uncertainty.} Such techniques often involve costly nested cross-validation where each available example is assigned all possible label states~\cite{roy:ml01, Moskovitch:2007util, Guo:2007optimistic}.

\subsection{Dealing with Class Imbalance Problem in Active Learning}

Selecting examples from an unlabeled pool with substantial class imbalance may pose several difficulties for traditional active learning. The greater proportion of examples in the majority class may lead to a model that prefers one class over another. If the labels of examples selected by an active learning scheme are thought of as a random variable, the innate class imbalance in the example pool would almost certainly lead to a preference for majority examples in the training set. Unless properly dealt with,\footnote{For instance, by imbalanced learning techniques described throughout this book.} this over-representation may simply lead to a predictive preference for the majority class when labeling. Typically, when making predictive models in an imbalanced setting, it is the minority class that is of interest. For instance, it is important to discover patients who have a rare but dangerous ailment based on the results of a blood test, or infrequent but costly fraud in a credit card company's transaction history. This difference in class preferences between an end-system's needs and a model's tendencies causes a serious problem for active learning (and predictive systems in general) in imbalanced settings. Even if the problem of highly imbalanced (though correct in terms of base rate) training set problem can be dealt with, the tendency for a selection algorithm to gather majority examples creates other problems. The nuances of the minority set may be poorly represented in the training data, leading to a ``predictive misunderstanding'' in certain regions of the problem space;  while a model may be able to accurately identify large regions of the minority-space, portions of this space may get mislabeled, or labeled with poor quality, due to underrepresentation in the training set. At the extreme, disjunctive sub-regions may get missed entirely. Both of these problems are particularly acute as the class imbalance increases, and are discussed in greater detail in Section~\ref{sec:disjunct}. Finally, in the initial stages of active learning, when the base model is somewhat na\"ive, the minority class may get missed entirely as an active learning heuristic probes the problem space for  elusive but critical minority examples.


\subsection{Addressing the Class Imbalance Problem with Active Learning}
As we will demonstrate in Section \ref{al_for_imbalanced_data}, active learning presents itself as an effective strategy for dealing with moderate class imbalance even, without any special considerations for the skewed class distribution. However, the previously discussed difficulties imposed by more substantial class imbalance on the selective abilities of active learning heuristics have led to the development of several techniques that have been specially adapted to imbalanced problem settings. These skew-specialized active learning techniques incorporate an innate preference for the minority class, leading to more balanced training sets and better predictive performance in imbalanced settings. Additionally, there exists a category of density-sensitive active learning techniques, techniques that explicitly incorporate the geometry of the problem space. By incorporating knowledge of independent dimensions of the unlabeled example pool, there exists a potential for better exploration, resulting in improved resolution of rare sub-regions of the minority class. We detail these two broad classes of active learning techniques below.

\subsubsection{Density-Sensitive Active Learning}

Utility-based selection strategies for active learning attribute some score, $\mathcal{U}(\cdot)$, to each instance $x$ encapsulating how much improvement can be expected from training on that instance. Typically, the examples offering a maximum $\mathcal{U}(x)$ are selected for labeling. However, the focus on individual examples may expose the selection heuristic to outliers, individual examples that achieve a high utility score, but do not represent a sizable portion of the problem space. Density-sensitive active learning heuristics seek to alleviate this problem by leveraging the entire unlabeled pool of examples available to the active learner. By explicitly incorporating the geometry of the input space when attributing some selection score to a given example, outliers, noisy examples, and sparse areas of the problem space may be avoided. Below are some exemplary active learning heuristics that leverage density sensitivity.

\textbf{Information Density}: this is a general density-sensitive paradigm compatible with arbitrary utility-based active selection strategies and a variety of metrics used to compute similarity~\cite{settles:2008sequence}. In this case, a meta-utility score is computed for each example based not online on a traditional utility score, $\mathcal{U}(x)$, but on a measurement of that example's similarity to other instances in the problem space. Given a similarity metric between two points, $\mbox{sim}(x,x')$, information density selects examples according to:

$$\mathcal{U}_m(x) = \mathcal{U}(x)\left(\frac{1}{|X|} \sum_{x' \in X} \mbox{sim}(x,x')\right)^\beta$$

Here, $\beta$ is a hyper-parameter controlling the tradeoff between the raw instance-specific utility, $\mathcal{U}(x)$ and the similarity component in the overall selection criterion.

Zhu et al.~\cite{zhuDensity2008} develop a similar technique to the information density technique of Settles and Craven, selecting instances according a uncertainty-based criterion modified by a density factor: $\mathcal{U}_n(x) = \mathcal{U}(x)\mbox{KNN}(x)$, where $\mbox{KNN}(x)$ is the average cosine similarity of the $K$ nearest neighbors to $x$. The same authors also propose \emph{sampling by clustering}, a density-only active learning heuristic where the problem space is clustered, and the points closest to the cluster centeroids are selected for labeling.

\textbf{Pre-Clustering}: here it is assumed that the problem is expressed as a mixture model comprising $K$ distributions, each component model completely encoding information related to the labels of member examples--- the label $y$ is conditionally independent of the covariates $x$ given knowledge of it's cluster, $k$~\cite{nguyen2004preclustering}. This assumption yields a joint distribution describing the problem: $p(x,y,k) = p(x|k)p(y|k)p(k)$, yielding a poster probability on $y$:

$$p_k(y|x)=\sum_{k=1}^K p(y|k)\frac{p(x|k)p(k)}{p(x)}$$

In essence, this a density weighted mixture model used for classification. The $K$ clusters are created by a application of typical clustering techniques of the data, with a cluster size used to estimate $p(k)$, and $p(y|k)$ is estimated via a logistic regression considering a cluster's representative example. A probability density is inferred for each cluster, in the example case presented in the above work, an  isotropic normal distribution is used, from which $p(x|k)$ can be estimated. Examples are then selected from an uncertainty score computed via the above posterior model weighted by the probability of observing a given $x$:

$$ \mathcal{U}_k(x) = \left( 1 - |p_k(y|x)|  \right)p(x) $$

Of course, there exists a variety of other techniques in the research literature designed to explicitly incorporate information related to the problem's density into an active selection criterion. McCallum and Nigam~\cite{cmccallum98em} modify a query-by-committee to use an exponentiated KL-divergence-based uncertainty metric and combine this with semi-supervised learning in the form of an expectation maximization procedure. This combined semi-supervised active learning has the benefit of ignoring regions that can be reliably ``filled in'' by a semi-supervised procedure, while also selecting those examples that may benefit this EM process.

Donmez et al.~\cite{donmez07dual} propose a modification of the density-weighted technique of  Nguyen and Smeulders. This modification simply selects examples according to the convex combination of the density weighted technique and traditional uncertainty sampling. This hybrid approach is again incorporated into a so-called dual active learner, where uncertainty sampling is only incorporated once the benefits of pure density sensitive sampling seem to be diminishing.

\textbf{Alternate Density-Sensitive Heuristics}: Donmez and Carbonell~\cite{donmez2008psd} incorporate density into active label selection by performing a change of coordinates into a space whose metric expresses not only euclidian similarity but also density. Examples are then chosen based on a density-weighted  uncertainty metric designed to select examples in pairs--- one member of the pair from  each side of the current decision boundary. The motivation is that sampling from both sides of the decision boundary may yield better results than selecting from one side in isolation.

Through selection based on an  ``unsupervised'' heuristic estimating the utility of label acquisition on the pool of unlabeled instances, Roy and McCallum incorporate the geometry of the problem space into active selection implicitly~\cite{roy:ml01}. This approach attempts to quantify the improvement in model performance attributable to each unlabeled example, taken in expectation over all label assignments:

$$\mathcal{U}_E = \sum_{y' \in Y} \hat{p}(y' | x)\sum_{x' \ne X}\mathcal{U}_e(x';x,y=y')$$

Here, the probability of class membership in the above expectation comes from the base model's current posterior estimates. The utility value on the right side of the above equation, $\mathcal{U}_e(x';x,y=y')$, comes from assuming a label of $y'$ for example $x$, and incorporating this pseudo-labeled example into the training set temporarily. The improvement in model performance with the inclusion of this new example is then measured. Since a selective label acquisition procedure may result in a small or arbitrarily biased set of examples, accurate evaluation through nested cross validation difficult. To accommodate this, Roy and McCallum propose two uncertainty measures taken over the pool of unlabeled examples, $x' \ne x$. Specifically, they look at the entropy of the posterior probabilities of examples in the pool, and the magnitude of the maximum posterior as utility measures, both estimated after the inclusion of the ``new'' example. Both metrics favor ``sharp'' posteriors, an optimization minimizing uncertainty rather than model performance; instances are selected by their reduction in uncertainty taken in expectation over the entire example pool.

\subsubsection{Skew-Specialized Active Learning}
\label{sec:skewal}

Additionally, there exists a body of research literature on active learning specifically to deal with class imbalance problem. Tomanek~\cite{tomanek2009imbalance} investigates query-by-committee-based approaches to sampling labeled sentences for the task of named entity recognition. The goal of their selection strategy is to encourage class-balanced selections by incorporating class-specific costs. Unlabeled instances are ordered by a class-weighted, entropy-based disagreement measure,     $ -\sum_{j \in \{0,1\}} b_j   \frac{V(k_j)}{|C|}\log \frac{V(k_j)}{|C|}$, where $V(k_j)$ is the number of                           votes from a committee of size $|C|$ that an instance belongs to a class   $k_j$. $b_j$ is a weight corresponding to the importance of including a  certain class; a larger value of $b_j$ corresponds to a increased tendency   to include examples that are thought to belong to this class. From a window  $W$ of examples with highest disagreement, instances are selected greedily   based on the model's estimated class membership probabilities so that the batch selected from the window has the highest probability of having a  balanced class membership.

SVM-based active learning has been shown~\cite{Ertekin2_2007} to be a highly effective strategy for addressing class imbalance without any skew-specific modifications to the algorithm. Bloodgood and Shanker~\cite{bloodgood2009imbalance} extend the benefits of SVM-based active learning by proposing an approach that incorporates class specific costs. That is, the typical $C$ factor describing an SVM's misclassification penalty is broken up into $C_+$ and $C_-$, describing costs associated with misclassification of positive and negative examples, respectively, a common approach for improving the performance of support vector machines in cost-sensitive settings. Additionally, cost sensitive support vector machines is known to yield predictive advantages in imbalanced settings by offering some preference to an otherwise overlooked class, often using the heuristic for setting class specific costs:  $\frac{C_+}{C_-} = \frac{|\{x | x \in -\}|}{|\{x | x \in +\}|}$, a ratio in inverse proportion to the number of examples in each class. However, in the active learning setting, the true class ratio is unknown, and the quantity  $\frac{C_+}{C_-}$ must be estimated by the active learning system. Bloodgood and Shanker show that it is advantageous to use a preliminary stage of random selection in order to establish some estimate of the class ratio, and then proceed with example selection according to the uncertainty-based ``simple margin'' criterion using the appropriately tuned cost-sensitive SVM.

\drop{
Zhu and Hovy~\cite{zhu2007imbalance} investigate active learning using uncertainty selection in conjunction with over and under-sampling to alleviate the class imbalance problem. Here active learning is used to choose a set of instances for labeling, with sampling strategies used to improve the class distribution. To alleviate this within-class imbalance while still providing the advantages of oversampling the minority class, the authors describe a bootstrap-based oversampling strategy (BootOS). At each epoch, the examples with the greatest uncertainty are selected for labeling and incorporated into a labeled set, $L$. From $L$, the proposed oversampling strategy is applied, yielding a more balanced data set, $L'$, a data set that is used to retrain the base model. The selection of the examples with the highest uncertainty for labeling at each iteration involves resampling the labeled examples and training a new classifier with the resampled dataset, therefore, scalability of this approach may be a concern for large scale datasets.
}

Active learning has also been studied as a way to improve the generalization performance of resampling strategies that address class imbalance. In these settings, active learning is used to choose a set of instances for labeling, with sampling strategies used to improve the class distribution. Ertekin~\cite{Ertekin_dissertation} presented \textsc{VIRTUAL}, a hybrid method of oversampling and active learning that forms an adaptive technique for resampling of the minority class instances. The learner selects the most informative example $x_i$ for oversampling, and the algorithm creates a synthetic instance along the direction of $x_i$'s one of $k$ neighbors. The algorithm works in an online fashion and builds the classifier incrementally without the need to retrain on the entire labeled dataset after creating a new synthetic example. This approach, which we present in detail in Section \ref{virtual}, yields an efficient and scalable learning framework. 

Zhu and Hovy~\cite{zhu2007imbalance} describe a bootstrap-based oversampling strategy (BootOS) that, given an example to be resampled, generates  a bootstrap example based on all the k neighbors of that example. At each epoch, the examples with the greatest uncertainty are selected for labeling and incorporated into a labeled set, $L$. From $L$, the proposed oversampling strategy is applied, yielding a more balanced data set, $L'$ , a data set that is used to retrain the base model. The selection of the examples with the highest uncertainty for labeling at each iteration involves  resampling the labeled examples and training a new classifier with the resampled dataset, therefore, scalability of this approach may be a concern for large scale datasets.


In the next section, we demonstrate that the principles of active learning are naturally suited to address the class imbalance problem and that active learning can in fact be an effective strategy to have construct a balanced view of an otherwise imbalanced dataset, without the need to resort to resampling techniques. It is worth noting that the goal of the next section is not to cast active learning as a replacement for resampling strategies. Rather, our main goal is to demonstrate how active learning can alleviate the issues that stem from class imbalance, and to present active learning as an alternate technique that should be considered in case a resampling approach is impractical, inefficient or ineffective. In problems where resampling \textit{is} the preferred solution, we show in Section \ref{virtual} that the benefits of active learning can still be leveraged to address class imbalance. In particular, we present an adaptive oversampling technique that uses active learning to determine which examples to resample in an online setting. These two different approaches show the versatility of active learning and the importance of selective sampling to address the class imbalance problem.

