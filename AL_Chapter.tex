\documentclass{wileySix}

\usepackage{color}


\usepackage{multirow}
\usepackage{rotating}
\usepackage{amsmath, amssymb}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{epsfig}
\usepackage{balance}
\usepackage{epstopdf}


\hyphenpenalty=5000
\tolerance=1000
\usepackage{float}
\floatstyle{ruled}
\newfloat{algo}{thp}{lop}
\floatname{algo}{Algorithm}
\newcommand{\josh}[1]{}%{{\bf [JOSH:{\em {#1}}]}}
\newcommand{\seyda}[1]{}%{{\bf [SEYDA:{\em {#1}}]}}

\newcommand {\drop}[1] {}
\setcounter{chapter}{6}


\begin{document}

\offprintinfo{Imbalanced Learning: Foundations, Algorithms, and Applications}{Haibo He and Yunqian Ma}

\chapter{Class Imbalance and Active Learning}
\chapterauthors{ Josh Attenberg$^\ast$ and \c{S}eyda Ertekin$^\dag$
\chapteraffil{$^\ast$Etsy; $\dag$Massachusetts Institute of Technology}
}
%	Josh Attenberg \\
%	Etsy \\
%	Brooklyn, NY 11201\\
%	\small\href{mailto:jattenberg@etsy.com}{\nolinkurl{jattenberg@etsy.com}}
%	\and
%	\c{S}eyda Ertekin \\
%	MIT Sloan School of Management \\
%    	Massachusetts Institute of Technology\\
%	Cambridge, MA 02142\\
%	\small\href{mailto:seyda@mit.edu}{\nolinkurl{seyda@mit.edu}}
%	}
\date{}

\noindent\textbf{Abstract}

The performance of a predictive model is tightly coupled with the set of data used in the training process. 
While having more examples available when training will often result in a better informed, more accurate model, 
memory limitations and real-world costs associated with gathering labeled examples often constrain just how 
many training data are available to be considered by a model's training process. In the setting where a only a limited
amount of training data is available during model induction, it often becomes meaninful to consider just which examples
are selected. In {\em Active learning}, the model itself places a hands-on role in the selection of examples for 
labeling, those examples to be used for training. Numerous studies have demonstrated emperically and
theoretically the benefits of active learning.; a training system that interactively involves the model being
trained can often result in a much more accurate model than simply selecting random examples for training, 
given a fixed budget.  Imbalanced settings provide special opportunities and challenges for active learning; 
while active learning can be used to build models that counteractact the harmful effects of learning under class
imbalance, extreme class imbalance can cause an active learning strategy to ``fail'', preventing the selection
scheme from choosing any useful examples for labeling. This chapter focuses on the interaction between actinve
learning and class imbalance, discussing i) active learning techniques designed specifically for dealing with 
imbalanced settings, ii) strategies that leverage active leaning to overcome the deleterious effects of class imbalance, 
iii) how extreme class imbalance can prevent active leaning systems from selecting useful examples, and alternatives
to active learning in these cases. 


\input{intro}
\input{active_background}
\input{svm_imbalance}
\input{adaptive}
%\input{seyda_tmp}
\input{skew_problem}
\input{starting_cold}
\input{alternatives}
\input{afl_and_ads}
\input{conclusion}


%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{ieeetr}
\bibliography{Ch7_reference.bib}
\end{document}

