\section{Conclusion}
\label{sec:conclusion}

This chapter presents a broad perspective on the relationship between active learning --- the selective acquisition of labeled examples for training statistical models --- and imbalanced data classification tasks where at least one of the classes in the training set is represented with much fewer instances than the other classes. Our comprehensive analysis of this relationship leads to the identification of two common associations, namely, $(i)$ the ability of active learning to deal with the data imbalance problem that, when manifested in a training set, typically degrades the generalization performance of an induced model, and $(ii)$ the impact class imbalance may have on the abilities of an otherwise reasonable active learning scheme to select informative examples, a phenomenon that is particularly acute as the imbalance tends towards the extreme.

Mitigating the impact of class imbalance on the generalization performance of an a predictive model, in Sections~\ref{al_for_imbalanced_data} and~\ref{virtual} we present active learning as an alternative to more conventional resampling strategies. An active learning strategy may select a data set that is both balanced and extremely informative in terms of model training. Early stopping criteria for halting the selection process of active learning can further improve the generalization of induced models. That is, a model trained on a small but informative subsample often offers performance far exceeding what can be achieved by training on a large dataset drawn from the natural, skewed base rate. The abilities of active learning to provide small, balanced training from large, but imbalanced problems are enhanced further by \textsc{Virtual}, introduced in Section~\ref{virtual}. Here, artificially generated instances supplement the pool of examples available to the active selection mechanism.

Additionally, it is noted throughout that abilities of an active learning system tend to degrade as the imbalance of the underlying distribution increases. While at more moderate imbalances, the quality of the resulting training set may still be sufficient to provide usable statistical models, but at more substantial class imbalances, it may be difficult for a system based upon active learning to produce accurate models. Throughout Sections~\ref{sec:related},~\ref{al_for_imbalanced_data}, and~\ref{virtual}, we illustrate a variety of active learning techniques specially adapted for imbalanced settings, techniques that may be considered by practitioners facing difficult problems. In Sections~\ref{sec:skew} and~\ref{sec:disjunct}, we note that as a problem's class imbalance tends towards the extreme, the selective abilities of an active learning heuristic may fail completely. We present several alternative approaches for data acquisition in Section~\ref{sec:other_settings}, mechanisms that may alleviate the difficulties active learning faces in problematic domains. Among these alternatives are guided learning and active class selection in Subsection~\ref{sec:acs}, and using associations between specific feature values and certain classes in Subsection~\ref{sec:feat}.

Class imbalance presents a challenge to statistical models and to machine learning systems in general. Because the abilities of these models are so tightly coupled with the data used for training, it is crucial to consider the selection process that generates this data. This chapter discusses specifically this problem. It is clear that when building models for challenging imbalanced domains, active learning is an aspect of the approach that shouldn't be ignored.
